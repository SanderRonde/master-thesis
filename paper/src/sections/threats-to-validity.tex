\chapter{Threats to Validity}\label{chap:threats-to-validity}

In this chapter, we cover threats to the validity of this study. We visit various categories of threats to validity, as laid out in the work performed by Wohling~\etal{}~\cite{alma9939160411205131}.

\section{Conclusion Validity}
A possible threat to conclusion validity would be the user study presented in Chapter~\ref{chap:discussion}. Drawing a conclusion from such a low sample size would lead to a high risk of an invalid conclusion. Instead, we opted to mark the resulting findings as mere indications to be proven in future work.

\section{Internal Validity}
An internal threat to validity could be the measurement of our metrics being influenced by external factors. As described in Section~\ref{sec:experimental-setup:load-time} we explicitly remove the factor of network speed from our benchmarks. This leaves only the factor of available system resources as a possible variable. In order to eliminate this factor, we took several steps. We first ensured a clean testing environment by shutting down all unneeded background processes on the test machine. This should vastly reduce the amount of fluctuation in available system resources. Secondly, we ensured that only a single test is running at a time. This means every test has the entire computer to itself (in practice, it likely only uses a single core) and does not compete with other tests for system resources. Lastly, we apply all the steps described in Section~\ref{sec:experimental-setup:time-sensitive-metrics} which includes randomizing the order in which the tests are run and increasing the number of tests to \numMeasures{} measurements per test. This should ensure that any possible fluctuations are smoothed out and shared across all tests.

\section{Construct Validity}
In order to ensure that the results of the experiments can be generalized to the created Web Component library, metrics that allow for the comparison of individual components have to be found. In order to validate the quality of components, we use a set of metrics that was validated through a user study by Martinez-Ortiz \etal{}~\cite{martinez-ortiz2016quality}, as described in Section~\ref{sec:related-work:metrics}. In addition, we use objective performance metrics that are independent of user perception. Gao \etal{}~\cite{gao2017perceived} describes such a problem for full web pages, where pages could be perceived as loaded while parts are still loading. To avoid such a situation, we have chosen three basic components that are either not loaded at all or fully loaded, namely the Button, Input, and Switch. This allows us to measure the loading time of individual components with a metric that is independent of user perception.

\section{External Validity}
A large number of the problems we face in this case study applied to Web Components in general, as shown in Section~\ref{sec:web-component-issues}. Still, a significant share of the problems is related to Angular, more specifically Angular 10. Likewise, a significant part of them is specific to the 30MHz codebase. Given a different codebase or a different Angular version, different problems might be faced in the process of converting Angular components to Web Components. While the issues might differ from framework to framework or even from Angular version to Angular version, the results should be generalizable to other frameworks and Angular versions. Bugs we encountered are likely to be fixed, and performance is likely to only improve in future Angular versions, as has been the case with the Angular Ivy compiler~\footurl{https://angular.io/guide/ivy}. Additionally, a significant amount of the faced problems apply to Web Components in general, as shown in Section~\ref{sec:web-component-issues}. For this reason, we believe that the feasibility of the applied process will at worst stay the same and at best improve for other Angular versions or JS frameworks.
Further, while the specific UI library we created is dependent mainly on the 30MHz codebase and its specific architecture and contents, we make sure to compare the created CC UI libraries with the original 30MHz codebase itself, ensuring all results are relative to the original. This should ensure we answer the research question for a generalized case. If we were to compare the CC UI library solely to other UI libraries, the answer to the research question would only apply to the case of 30MHz.